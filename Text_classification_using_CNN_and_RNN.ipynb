{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_classification_using_CNN_and_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ISPGMrvOVcu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4384b4a2-05a7-4f2c-ef4b-4bc1aade8ec2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVDD4ym3OeIc",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exKe_i3dOdZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQB1ZwygOm4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=\"drive/My Drive/IMDB Dataset.csv\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDxuV_mVOoOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(dataset)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hotys9OaOpdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "import re\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWlXMJN8OwiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review']=df['review'].apply(denoise_text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS7lkZOXOzep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpIgW87fOzT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Convert_to_bin(text, remove_digits=True):\n",
        "    if text=='positive':\n",
        "      text= 1\n",
        "    else:\n",
        "      text=0\n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndtRuY5jO3SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review']=df['review'].apply(remove_special_characters)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oBR1ZhAO55d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['sentiment']=df['sentiment'].apply(Convert_to_bin)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbTNe4BO7rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPTlnBqZO-IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=df['review'].values\n",
        "Y=df['sentiment'].values\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoL9d59POLe",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi5VYQr5PgNo",
        "colab_type": "text"
      },
      "source": [
        "In mathematics (in particular, functional analysis) convolution is a mathematical operation on two functions (f and g) that produces a third function expressing how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it.\n",
        "\n",
        "So, Convolutional is best for extracting special features and behaviour of feature values from the 2D pixels of images. Convolutional layers hava a set of kernels which helps to extract several important features from the data samples. Now here, in text classifications our feature matrices are 1Dimensional. So, here Conv1D is used. Basically it moves as a sliding window of size decieded by the user. We have chosen 5. \n",
        "\n",
        "Now, initially after embedding we get 100 Dimensional embedding. Next using 1D convolutions we try to make our feature set smaller and let the feature set dicover the best features relations for the classification. The maxpooling layer also helps to pick the features or words which have best performance. \n",
        "\n",
        "\n",
        "Convolutional layer is always used after an embedding layer after it provides its embedded feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvvS6EinPV-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPGuPs9-Qqf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=10000)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-2KN9chQrzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZeAiZr3QtHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = tokenizer.texts_to_sequences(X_train) "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsPquE65QvLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu4jW4i1Qvtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = len(tokenizer.word_index) + 1  "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uscYv130QxYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2dcosu4Q0Ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 100"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vpq_2wMQ2Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhyNWH0IQ5f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Dense, Activation, MaxPool1D,Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "emb_dim=100\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(input_dim=vocab, output_dim=emb_dim, input_length=maxlen))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPool1D(5))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPool1D(5))\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jwuQrcuRhJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "ca1287d8-3896-4b2f-b6d0-f6fe4e2e67d4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 100)          17739900  \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 96, 64)            32064     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 19, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 15, 128)           41088     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 3, 128)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3, 16)             2064      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3, 1)              17        \n",
            "=================================================================\n",
            "Total params: 17,815,133\n",
            "Trainable params: 17,815,133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHVdmxlPRorL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "b5f112fc-3cf4-49e2-fde8-2bfbac0f0bef"
      },
      "source": [
        "history = model.fit(x_train, Y_train,epochs=20,verbose=True,batch_size=16)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2188/2188 [==============================] - 366s 167ms/step - loss: 0.0466 - accuracy: 0.9805\n",
            "Epoch 2/20\n",
            "2188/2188 [==============================] - 366s 167ms/step - loss: 0.0341 - accuracy: 0.9851\n",
            "Epoch 3/20\n",
            "2188/2188 [==============================] - 366s 167ms/step - loss: 0.0288 - accuracy: 0.9869\n",
            "Epoch 4/20\n",
            "2188/2188 [==============================] - 368s 168ms/step - loss: 0.0259 - accuracy: 0.9880\n",
            "Epoch 5/20\n",
            "2188/2188 [==============================] - 370s 169ms/step - loss: 0.0223 - accuracy: 0.9892\n",
            "Epoch 6/20\n",
            "2188/2188 [==============================] - 368s 168ms/step - loss: 0.0228 - accuracy: 0.9895\n",
            "Epoch 7/20\n",
            "2188/2188 [==============================] - 370s 169ms/step - loss: 0.0221 - accuracy: 0.9897\n",
            "Epoch 8/20\n",
            "2188/2188 [==============================] - 370s 169ms/step - loss: 0.0202 - accuracy: 0.9902\n",
            "Epoch 9/20\n",
            "2188/2188 [==============================] - 370s 169ms/step - loss: 0.0183 - accuracy: 0.9909\n",
            "Epoch 10/20\n",
            "2188/2188 [==============================] - 372s 170ms/step - loss: 0.0192 - accuracy: 0.9908\n",
            "Epoch 11/20\n",
            "2188/2188 [==============================] - 374s 171ms/step - loss: 0.0166 - accuracy: 0.9917\n",
            "Epoch 12/20\n",
            "2188/2188 [==============================] - 372s 170ms/step - loss: 0.0169 - accuracy: 0.9916\n",
            "Epoch 13/20\n",
            "2188/2188 [==============================] - 371s 170ms/step - loss: 0.0158 - accuracy: 0.9918\n",
            "Epoch 14/20\n",
            "2188/2188 [==============================] - 371s 170ms/step - loss: 0.0154 - accuracy: 0.9922\n",
            "Epoch 15/20\n",
            "2188/2188 [==============================] - 371s 170ms/step - loss: 0.0147 - accuracy: 0.9924\n",
            "Epoch 16/20\n",
            "2188/2188 [==============================] - 375s 171ms/step - loss: 0.0146 - accuracy: 0.9924\n",
            "Epoch 17/20\n",
            "2188/2188 [==============================] - 371s 170ms/step - loss: 0.0138 - accuracy: 0.9928\n",
            "Epoch 18/20\n",
            "2188/2188 [==============================] - 370s 169ms/step - loss: 0.0137 - accuracy: 0.9928\n",
            "Epoch 19/20\n",
            "2188/2188 [==============================] - 371s 169ms/step - loss: 0.0147 - accuracy: 0.9925\n",
            "Epoch 20/20\n",
            "2188/2188 [==============================] - 369s 169ms/step - loss: 0.0130 - accuracy: 0.9931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWa74PxZhARa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c91896de-d81f-4ded-ed93-3d327e8e6ac5"
      },
      "source": [
        "test_score=model.evaluate(x_test,Y_test)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 2s 3ms/step - loss: 1.7621 - accuracy: 0.7642\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.7621 - accuracy: 0.7642\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.7621 - accuracy: 0.7642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8YSLfRmhCXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad274645-3952-4202-8634-24eae4664bef"
      },
      "source": [
        "test_score"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7621402740478516, 0.7641777396202087]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7621402740478516, 0.7641777396202087]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7621402740478516, 0.7641777396202087]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhK1Vh-OhDym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5020ad4c-b682-4977-f1b7-bad9920bcfe7"
      },
      "source": [
        "train_score=model.evaluate(x_train,Y_train)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1094/1094 [==============================] - 4s 3ms/step - loss: 0.0111 - accuracy: 0.9938\n",
            "1094/1094 [==============================] - 4s 3ms/step - loss: 0.0111 - accuracy: 0.9938\n",
            "1094/1094 [==============================] - 4s 3ms/step - loss: 0.0111 - accuracy: 0.9938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3NCsR4XhHHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7904d481-83e7-4cf1-d1f3-e7646be63906"
      },
      "source": [
        "train_score"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01113097183406353, 0.9938176274299622]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01113097183406353, 0.9938176274299622]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01113097183406353, 0.9938176274299622]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZjKcz-Q2kKr",
        "colab_type": "text"
      },
      "source": [
        "### Train accuracy 0.99\n",
        "### Test accuracy  0.77"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWb6nJIZ0Tz",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Network application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0hhyhulaYag",
        "colab_type": "text"
      },
      "source": [
        "**Why Recurrent Neural Networks**\n",
        "\n",
        "Until now we have tried to extract some features from all the words in a sample at a time. So, all of them are non temporal approaches. Now, let's see how a person will judge a sentiment. He/she will not only considered what were the words used, humans will also consider how they are used, that is, in what context, and what are the preeceding and succeeding words? So, until now we have focused on what were the words used only, so, now let's look at the other part of the story. \n",
        "\n",
        "So, for this part we need Recurrent neural network to give a memory to our models. If we think about telling something about someones statements, we will generally listen to the whole statement word by word and then make a comment. This is what the Recurrent Neural networks will accomplish. It will look at each word on a temporal manner one by one and try to correlate to the context using the embedded feature vector of the word. \n",
        "\n",
        "Now as we know RNN suffers from the vanishing and exploding gradient problem we will be using LSTM. \n",
        "\n",
        "Now LSTM, operates on two things a hidden state that is sent from previous time stamp and a cell state that actually maintains the weight neutralizing the vanishing gradient effect.\n",
        "\n",
        "Now, the LSTM layer basically has 4 components:\n",
        "A Forget gate, An input gate, a cell state and a output gate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WOuw5kb0bs2",
        "colab_type": "text"
      },
      "source": [
        "![LSTM](https://miro.medium.com/max/700/1*-kBdBYzR7lpimgb3AIRkOw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZP347bL1J89",
        "colab_type": "text"
      },
      "source": [
        "![gates](https://miro.medium.com/max/700/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9MWWPJ03YUc",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/700/1*y-RI3y90IZpOUMnkCBrQxQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KY5kBPK32d4",
        "colab_type": "text"
      },
      "source": [
        "**LSTM also provide feature set on the last time stamp for the dense layer to use the feature set to produce results.**  Now we can see the above equations are the equations for the Gates of LSTM. Now here each gate acts like a neural network individually. So, they have their individual weight matrices that are optimized when the recurrent network model is trained. Using these weight matrices only the gates learn their tasks, like which data to foget and what part of the data is needed to be updated to the cell state. So, the gates optimize their weight matrices and decide the operations accoring to it. \n",
        "\n",
        "Now, lets see the use. \n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Say we have a 100 dimensional vector space. a batch size of 16, each sample length = 10. and the number of nodes in each layer= 64.\n",
        "\n",
        "INPUT SIZE = batch_size * Embedding \n",
        "so, here it is 16 x 300 matrix = i(t)\n",
        "\n",
        "The time stamp 0 that is the first word of every sample enter. \n",
        "\n",
        "PREVIOUS HIDDEN STATE (0 vec for tiemstamp 0) = Batch size x Hidden Units \n",
        "So, Here it is 16 x 64 matrix.= h(t-1)\n",
        "\n",
        "After concatenation, the matrix formed h(t-1)= 16 x 64 and i(t)= 16 x 300\n",
        "\n",
        "So the h(t-1) + i(t) matrix is sent to all gates.\n",
        "\n",
        "--------------------------------------------------\n",
        "First the forget gate Weight matrix of the hidden state is of dimension 64 x 64 because in the hidden state for each of the 16 words of timestamp (t-1) there were 64 values from the 64 nodes from the RNN. \n",
        "\n",
        "So, actually our matrix from hidden state 16 rows which are records and for each record there are 64 columns or 64 features. \n",
        "\n",
        "y=w1x1+w2x2+........wnxn \n",
        "\n",
        "where the x's are the features or the column values. So, there must a maintained array of 64 weights for each node or unit of the network. Now there are 64 such units so total of (64 x 64) matrix.\n",
        "\n",
        "Again, now for input, there are 16 rows or records for each of them 300 columns or 300 features. so, the weight matrix of one hidden unit must have 100 values. Total 64 units are there. So, dimension of the matrix. (100 x 64)\n",
        "\n",
        "So at forget gate \n",
        "\n",
        "y= sigmoid((16 x 100) x (100 x 64) + (16 x 100)  x (100 x 64))\n",
        "\n",
        "y=sigmoid (16 x 64) vector\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Sigmoid gives the value between 0 and 1. If the value is close to 0 the value is forgotten else added to the cell state.\n",
        "\n",
        "Now,the cell state is also of the same dimension (16 x 64) as it is also having the weights of the 16 sample word's by 64 nodes So, they can easily be added.\n",
        "\n",
        "\n",
        "\n",
        "Next is the input gate it decides what part of the data should enter means the actual tanh. It deciedes whether the cell state should be updated. \n",
        "\n",
        "These gates matrices are also same as the forget gates matrices with (64 x 64) values in the last hidden layer and (300 x 64) values for the input.\n",
        "\n",
        "So, they also give sigmoid(16 x 64) as a result\n",
        "\n",
        "One thing to notice here is there is a tanh layer also. The tanh is here to squeeze the value between 1 to -1 to deal with the exploding and vanishing gradient. So, it basically works like and regularized value that represnts the value of the cell state on that timestep. The sigmoid is the switch. \n",
        "\n",
        "So, after that the obtained vectors are just multiplied to obtain 1 result.\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "**One thing to notice about this is, though the wieght matrices are of same dimensions they are not same. They belong to different gates and their values and optimzations are all different.**\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Now in the next step the cell step is updated \n",
        "\n",
        "It is basically a simple sum. \n",
        "\n",
        "The new c is formed by removing the unwanted information from the last step + accomplishments of the current time step. \n",
        "\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Next, comes the output gate. This decides what should be next steps hidden layer be. \n",
        "\n",
        "\n",
        "For this the new cell state is passed through a tanh gate and the h(t-1) + i(t) is passed through another sigmoid. Both of the results are multiplied. That's the next.\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "\n",
        "Now, these weights get updated at every timestep with every word and after the 10th word or timestamp. We get a matrix of size 16 x 64, which are basically the weight values of the nodes corresponding to each sample. But what we don't see are the weight matrices of the gates but they are also optimized. These 64 values basically represent the weights of a sample in the batch. \n",
        "\n",
        "For all the samples we obtain a value. These values act as feature set for the dense layers to perform their operations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSZIKWLaO3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}