{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_classification _sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "632HsFyhPgk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b155eeca-86e4-4260-fbb9-e55b8c1838d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PoDhi3QbwC",
        "colab_type": "text"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBol542lQBVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucFEVOV0QsU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=\"drive/My Drive/IMDB Dataset.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKrCEswcZQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhRwjblSmRgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "22daf51c-0b7b-415f-af64-95af43cf4085"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgms4oCRmbwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2f55dd90-8449-4d55-f0e6-c1597dca42e1"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rRFzWDXmfC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a80f5db8-801b-4ff8-88de-86169cb635e9"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BnvR11cmlo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWoFj8yhw5LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_64NFmI9w8te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCqSniK8xFpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review']=df['review'].apply(denoise_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NilyoOqxn1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "522dc7a9-e3c7-45cd-db0f-ca0bd90bb5f8"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. The filming tec...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played Shakespeare.Shakespeare lost.I...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade. T...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this film,it was the first film i h...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film! It must have been up against so...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review sentiment\n",
              "0   One of the other reviewers has mentioned that ...  positive\n",
              "1   A wonderful little production. The filming tec...  positive\n",
              "2   I thought this was a wonderful way to spend ti...  positive\n",
              "3   Basically there's a family where a little boy ...  negative\n",
              "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5   Probably my all-time favorite movie, a story o...  positive\n",
              "6   I sure would like to see a resurrection of a u...  positive\n",
              "7   This show was an amazing, fresh & innovative i...  negative\n",
              "8   Encouraged by the positive comments about this...  negative\n",
              "9   If you like original gut wrenching laughter yo...  positive\n",
              "10  Phil the Alien is one of those quirky films wh...  negative\n",
              "11  I saw this movie when I was about 12 when it c...  negative\n",
              "12  So im not a big fan of Boll's work but then ag...  negative\n",
              "13  The cast played Shakespeare.Shakespeare lost.I...  negative\n",
              "14  This a fantastic movie of three prisoners who ...  positive\n",
              "15  Kind of drawn in by the erotic scenes, only to...  negative\n",
              "16  Some films just simply should not be remade. T...  positive\n",
              "17  This movie made it into one of my top 10 most ...  negative\n",
              "18  I remember this film,it was the first film i h...  positive\n",
              "19  An awful film! It must have been up against so...  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7AWY8aww-Pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRVvCJBTbgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Convert_to_bin(text, remove_digits=True):\n",
        "    if text=='positive':\n",
        "      text= 1\n",
        "    else:\n",
        "      text=0\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69m6G5iNxx8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review']=df['review'].apply(remove_special_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG6v5aAQTu1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['sentiment']=df['sentiment'].apply(Convert_to_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMegnBOex5MM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "0415a58d-3f00-4d7c-e832-c5445567a8c2"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production The filming tech...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically theres a family where a little boy J...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my alltime favorite movie a story of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing fresh  innovative ide...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Bolls work but then aga...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played ShakespeareShakespeare lostI a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes only to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade Th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this filmit was the first film i ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film It must have been up against som...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review  sentiment\n",
              "0   One of the other reviewers has mentioned that ...          1\n",
              "1   A wonderful little production The filming tech...          1\n",
              "2   I thought this was a wonderful way to spend ti...          1\n",
              "3   Basically theres a family where a little boy J...          0\n",
              "4   Petter Matteis Love in the Time of Money is a ...          1\n",
              "5   Probably my alltime favorite movie a story of ...          1\n",
              "6   I sure would like to see a resurrection of a u...          1\n",
              "7   This show was an amazing fresh  innovative ide...          0\n",
              "8   Encouraged by the positive comments about this...          0\n",
              "9   If you like original gut wrenching laughter yo...          1\n",
              "10  Phil the Alien is one of those quirky films wh...          0\n",
              "11  I saw this movie when I was about 12 when it c...          0\n",
              "12  So im not a big fan of Bolls work but then aga...          0\n",
              "13  The cast played ShakespeareShakespeare lostI a...          0\n",
              "14  This a fantastic movie of three prisoners who ...          1\n",
              "15  Kind of drawn in by the erotic scenes only to ...          0\n",
              "16  Some films just simply should not be remade Th...          1\n",
              "17  This movie made it into one of my top 10 most ...          0\n",
              "18  I remember this filmit was the first film i ha...          1\n",
              "19  An awful film It must have been up against som...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSkWQBn0x9hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtIgQ4BF1UP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=df['review'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrztROLo1ZCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=df['sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lyuWULG1fx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "0b51f3c9-bf10-466d-f0fc-3c550c13b652"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(df['sentiment'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f458012fef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chwNb6zH1zMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izqKYrYpdrs5",
        "colab_type": "text"
      },
      "source": [
        "## Artificial Neural Networks Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mSsDCypOIij",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizers :Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88RS0HPQ27oC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Vectorizers**: In this method, we create a single feature vector using all the words in the vocubulary. Each word is basically  reagrded as a feature. So, the number of features is equal to the number of unique words in the vocab. Now, each sentence or review is a sample or record. Now, if the word is present in that sample it has some values and if the word is not present it is zero. Also called bag of words model\n",
        "\n",
        "So, each sample has the same feature set size which is equal to the size of the vocabulary. Now, the vocabuary is basically made of the words in the train set. All the samples of the train and test set is fit using this vocabulary only. So, there may be some words in the test samples which are not present in the vocabulary, they are ignored. \n",
        "\n",
        "Now, they form very sparse matrices or feature sets. Now, similar to a normal classification problem, the words become features of the record and the corresponding tag becomes the target value. So, it is actually like a common classification problem with number of features being equal to the distinct tokens in the training set.\n",
        "\n",
        "This can be done in two ways:\n",
        "\n",
        "\n",
        "\n",
        "1. Count Vectorizer: Here the count of a word in a particular sample or review. The count of that word becomes the value of the corresponding word feature. If a word in the vocab does not appear in the sample its value is 0. \n",
        "\n",
        "2. TF-IDF Vectorizer: It is a better approach. It calculates two things term frequency and inverse document frequency. Term frequency= No. of times the word appears in the sample. IDF = log ( number of time the word appears in the sample / number of time the word appears in the whole document). This helps to note some differences like the word \"The\" appears with same freq in almost all sentences while special words carrying significance like \"good\" don't. So, these TF and IDF terms are multiplied to obtain the vector formats for each sample. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60lfWluhOYTn",
        "colab_type": "text"
      },
      "source": [
        "#### Count Vectorizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIOFMb_62Iql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS7erqyzQUuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcIyRZ3uQaC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1525cd74-fbd7-46fe-b134-85bb7cecfa29"
      },
      "source": [
        "vec.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnBFYSHtQzNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=vec.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_bIhFMZQ6bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=vec.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jnXyYWtRIN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "09878b8b-26a2-4631-bd42-dcfe61654804"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<35000x177470 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 4767914 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjzM40v0ROnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=x_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYQWyJZQSZm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5fcd9200-b27b-4f7d-ae7c-c314c5d74ffb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                2839536   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,839,825\n",
            "Trainable params: 2,839,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOuW2LfUSdXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9399492-f408-4f74-bb33-abe828220594"
      },
      "source": [
        "history = model.fit(x_train, Y_train,epochs=100,verbose=True,batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.3146 - accuracy: 0.8744\n",
            "Epoch 2/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.1149 - accuracy: 0.9576\n",
            "Epoch 3/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0499 - accuracy: 0.9821\n",
            "Epoch 4/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0287 - accuracy: 0.9897\n",
            "Epoch 5/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0186 - accuracy: 0.9942\n",
            "Epoch 6/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0137 - accuracy: 0.9959\n",
            "Epoch 7/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0078 - accuracy: 0.9974\n",
            "Epoch 8/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0077 - accuracy: 0.9977\n",
            "Epoch 9/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0071 - accuracy: 0.9978\n",
            "Epoch 10/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0025 - accuracy: 0.9993\n",
            "Epoch 11/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0050 - accuracy: 0.9985\n",
            "Epoch 12/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0029 - accuracy: 0.9991\n",
            "Epoch 13/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0026 - accuracy: 0.9993\n",
            "Epoch 14/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0020 - accuracy: 0.9995\n",
            "Epoch 15/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0021 - accuracy: 0.9992\n",
            "Epoch 16/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0019 - accuracy: 0.9993\n",
            "Epoch 17/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0018 - accuracy: 0.9995\n",
            "Epoch 18/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.8614e-04 - accuracy: 0.9999\n",
            "Epoch 19/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9998\n",
            "Epoch 20/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0036 - accuracy: 0.9992\n",
            "Epoch 21/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0019 - accuracy: 0.9995\n",
            "Epoch 22/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.0364e-04 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.4194e-05 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.0649e-04 - accuracy: 0.9999\n",
            "Epoch 25/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.9919e-05 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.9021e-05 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 4.1639e-05 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.3546e-05 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0054 - accuracy: 0.9995\n",
            "Epoch 30/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0040 - accuracy: 0.9994\n",
            "Epoch 31/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0017 - accuracy: 0.9995\n",
            "Epoch 32/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.9993\n",
            "Epoch 33/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 8.0544e-05 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.6258e-05 - accuracy: 0.9999\n",
            "Epoch 35/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.6887e-05 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 4.5846e-05 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.5241e-05 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.1134e-04 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 4.3324e-05 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 2.5094e-05 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0028 - accuracy: 0.9994\n",
            "Epoch 42/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0026 - accuracy: 0.9994\n",
            "Epoch 43/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9997\n",
            "Epoch 44/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.2710e-05 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 2.4858e-05 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 2.2871e-05 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.5169e-05 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.9690e-05 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.8087e-05 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.7954e-05 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.4394e-05 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.3059e-05 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.2217e-05 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.0851e-05 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 2.2537e-05 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.5112e-06 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.1919e-06 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.1334e-06 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.6236e-07 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9997\n",
            "Epoch 61/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 0.0068 - accuracy: 0.9991\n",
            "Epoch 62/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.9522e-04 - accuracy: 0.9999\n",
            "Epoch 63/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.1697e-07 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 4.8724e-07 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.2893e-07 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.8410e-07 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 8.6728e-08 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 4.5799e-08 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.7244e-08 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.2370e-09 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 3.7562e-09 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.6814e-09 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 8.2362e-10 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 4.5086e-10 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 2.9417e-10 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 2.0863e-10 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.6438e-10 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.3974e-10 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.2130e-10 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 1.1140e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 9.6768e-11 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 9.0848e-11 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 9.1615e-11 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 8.6676e-11 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.6896e-11 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.8929e-11 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.5444e-11 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.0275e-11 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.5885e-11 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 7.0733e-11 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.1445e-11 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.4795e-11 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.4106e-11 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.8243e-11 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.1660e-11 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.3604e-11 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.3272e-11 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.2296e-11 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 5.3251e-11 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "2188/2188 [==============================] - 6s 3ms/step - loss: 6.2355e-11 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmwg2o6jX3OH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ff25970e-0b55-40b2-934d-80aed59abbaa"
      },
      "source": [
        "model.evaluate(x_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1094/1094 [==============================] - 2s 2ms/step - loss: 6.4925e-11 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.492464899032768e-11, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc0kiSG8YF0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fdce4b20-c346-4681-885b-b4c7a1bb6dd5"
      },
      "source": [
        "model.evaluate(x_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 1s 2ms/step - loss: 3.1591 - accuracy: 0.8731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.1590538024902344, 0.8730666637420654]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPhmwiB3YLVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYEFF4DFYNXB",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ljhTBIYT5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl5Dz-kbZCC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxz_qjmTZGG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e7b64795-5053-42e4-e993-764617f27315"
      },
      "source": [
        "vec.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqlHfFJCZJVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=vec.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imtf8G4fZNM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=vec.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVwpPy90ZPd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19e29109-5be9-40ff-8ed2-77d8c20dc407"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<35000x176924 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 4778597 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHpCJ9WOZQ3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2a96257b-f629-4d40-abc3-8bec434c966d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2pJELG_aIKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_test = lr.score(x_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHInK48EdBmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ef3ab97-c709-44fa-9893-7674fdf335ed"
      },
      "source": [
        "score_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8941333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV4pACNLdD9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_train = lr.score(x_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt9-DhnXdPk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3466d1b0-1bac-410d-c13e-a2ddc51d6f58"
      },
      "source": [
        "score_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9325428571428571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3XGRpKhdT0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnFjVonjdTIJ",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding : Time series model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Zed2gpdz5K",
        "colab_type": "text"
      },
      "source": [
        "**Word Embedding**: In this method, the words are individually represented as a vector. In case of the bag of words all of the words made up a vector. Here, there are 100 words in a vocabulary, so, a specific word will be represented by a vector of size 100 where the index corresponding to that word will be equal to 1, and others will be 0. \n",
        "\n",
        "So, Each sample having different number of words will basically have a different number of vectors, as each word is equal to a vector. Now, to feed a model we will need to have the same dimension for each sample, and as a result padding is needed to make the number of words in each sample equal to each other.  \n",
        "\n",
        "Basically in the bag of words or vectorizer approach, if we have 100 words in our total vocabulary, and a sample with 10 words and a sample with 15 words, after vectorization both the sample sizes would be an array of 100 words, but here for the 10 words it will be a (10 x 100) i,e 100 length vector for each of the 10 words and similarly for 15th one size will be (15 x 100). So, we need to find the longest sample and pad all others up to match the size.\n",
        "\n",
        "We can do this in some ways:\n",
        "\n",
        "One-Hot encoding: It is just taking the size of the vocabulary and making an array of that size with 0's at all indices and 1 at only the index of the word. But this things provides us with a very less information.\n",
        "\n",
        "The Second Choice is word embeddings.\n",
        "\n",
        "The one hot encoder is a pretty hard coded approach. It is of a very high dimension and sparse with a very low amount of data. **Embedding is a way to create a dense vector representation out of the sparse representations.** It is of a lower dimension and helps to capture much more informations. It more like captures the relation and similarities between words using how they appear close to each other. For example, king, queen, men and women will have some relations. \n",
        "\n",
        "Say, we are having 10k words are being embedded in a 300 dimensional embedding space. To do this, we declare the number of nodes in the embedding layer =300. Now, each word of the 10k words enter the embedding layer as a 10k sized individual vector, Now, each of the words will be placed in 300 dimensional plane based on their similarities with one another which is decieded by several factors, like the order in which the words occur. Now, being placed in 300 Dimensional plane the words will have a 300 length tuple to represent it which are actually the coordinates of the point on the 300 dimensional plane. So, this 300 dimensional tuple becomes the new feature set or representing vector for the word. \n",
        "\n",
        "So, the vector for the word decreased from 10k to 300. The tuples serve as feature vectors between two words and the cosine angle between the vectors represent the similarity between the two words.\n",
        "\n",
        "We can do this in two ways:\n",
        "\n",
        "1. Using our own embeddings\n",
        "\n",
        "2. Using pretrained embeddings\n",
        "\n",
        "**Making our own embedding** \n",
        "\n",
        "Now, for the embedding we need to enter the sparse feature vectors first then move to making them dense using embedding. \n",
        "\n",
        "For creating the sparse matrix here, we need to use a tensorflow tokenizer, tokenizes the text to tokens. It can be done in mainly three ways:\n",
        "1. Binary: X = tokenizer.sequences_to_matrix(x, mode='binary') : In this case, its simply like one hot encoding, 1 for the word 0 for others\n",
        "\n",
        "2. Count: X = tokenizer.sequences_to_matrix(x, mode='count') : In this case, it is the number of times a word appear in the sentence.\n",
        "\n",
        "3. TF-IDF: X  = tokenizer.sequences_to_matrix(x, mode='tfidf') : In this we consider the TF of the word in the sample and IDF of the word in the sample with resect to the occurence of the word in the whole document.  \n",
        "\n",
        "Then we feed the vectors to the embedding layer. \n",
        "\n",
        "**Using pretrained embedded matrices**\n",
        "\n",
        "We can use pretrained word embeddings like word2vec by google and GloveText by standford.They are trained on huge corpuses with billions of examples and words. Now, they have billions of words we have only a 10k so, training our model with a billion words will be very inefficient, So, we need to just select out our required word's embeddings from their pretrained embeddings.\n",
        "\n",
        "Now, **How are these embeddings found?** \n",
        "\n",
        "For google's word2vec implementations, there are two ways:\n",
        "\n",
        "1. Continous bag of words\n",
        "2. Spin Gram.\n",
        "\n",
        "Both of these algorithms actually use a Neural Network with a single hidden layer to generate the embedding.\n",
        "\n",
        "Now, for CBOW, the context of the words , i.e, the words befor and after the required words are fed to the neural network, and the model is needed to predict the word. \n",
        "\n",
        "For the Spin-Gram, the words are given and the model has to predict the context words.\n",
        "\n",
        "In both cases, the feature vectors or encoded vectors of the words are fed to the input. The output has a softmax layer with number of nodes equal to the vocabulary size, which gives the percentage of prediction for each word. Though we don't use the output layer actually. \n",
        "\n",
        "We go for the weight matrix produced in the hidden layer. **The number of nodes in the hidden layer is equal to the embedding dimension.** So, say if there are 10k words in a vocabulary and 300 nodes in the ghidden layer, each node in the hidden layer will have an array of weights of dimension of 10k for each word after training. \n",
        "\n",
        "Because the neural network units work on\n",
        "\n",
        "y=f(w1x1+w2x2+..........wnxn)\n",
        "\n",
        "Here x1, x2...... xn are the words and so n= number of words in vocabulary=10k. \n",
        "\n",
        "So for 10k x's there will be 10k w's. Now foe 1 node there are 10k length weight matrix. For 300 combined we have a matrix of 300 x 10k wieghts. Now, if we concatenate, we will have 300 rows and 10k columns. Let's transpose the matrix. We will get 300 columns and 10k rows. Each row represents a word, and the 300 column values represent a 300 length wieght vector for that word.\n",
        "\n",
        "This weight vector is the obtained embedding of length 300."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxI40eQr4rXH",
        "colab_type": "text"
      },
      "source": [
        "### Training own vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuOGt-ewdjdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95ec5dca-05ef-41de-a338-cb56613e605c"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSf0hC6B4gP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXaZtg524qb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbzweVge5ufl",
        "colab_type": "text"
      },
      "source": [
        "1) tokenize.fit_on_text() -->> Creates the vocabulary index based on word frequency. For example, if you had the phrase \"My dog is different from your dog, my dog is prettier\", word_index[\"dog\"] = 0, word_index[\"is\"] = 1 (dog appears 3 times, is appears 2 times)\n",
        "\n",
        "2) tokenize.text_to_sequence() -->> Transforms each text into a sequence of integers. Basically if you had a sentence, it would assign an integer to each word from your sentence. You can access tokenizer.word_index() (returns a dictionary) to verify the assigned integer to your word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XrCNMJr41q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = tokenizer.texts_to_sequences(X_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5PYo-J650wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc9Xay2m59XS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = len(tokenizer.word_index) + 1  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBAOrVYYDX1b",
        "colab_type": "text"
      },
      "source": [
        "+1 for padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6jHFS-lDfh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkEKBrLnDjRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwWpYRT6DlL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k4LumHKD6WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Dense, Activation, MaxPool1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "emb_dim=100\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(input_dim=vocab, output_dim=emb_dim, input_length=maxlen))\n",
        "model.add(MaxPool1D())\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oR8iq7BG11y",
        "colab_type": "text"
      },
      "source": [
        "Maxpooling is used to convert the sparse matrix denser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXOjlbEmGvtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "45347728-3775-4858-f6ab-7c6a055a18f7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 100, 100)          17754200  \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 50, 16)            1616      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 50, 16)            272       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 50, 1)             17        \n",
            "=================================================================\n",
            "Total params: 17,756,105\n",
            "Trainable params: 17,756,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZLFWwdRHH8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, Y_train,epochs=35,verbose=True,batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDu4vJSjNb4R",
        "colab_type": "text"
      },
      "source": [
        "Here at each step the model weights as well as the embedding dimensions are getting corrected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb37PPwAgEXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_score=model.evaluate(x_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QleSGiQg1mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4BATeY_g1cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_score=model.evaluate(x_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkMa8PZPg80J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi_BXOjc_j8q",
        "colab_type": "text"
      },
      "source": [
        "### Train accuray 83.7\n",
        "### Test accuracy 72.4"
      ]
    }
  ]
}